"""
Syndgen Pipeline Core Module

Implements the tri-stage pipeline architecture:
1. Seed Layer
2. Inference Layer (Generator)
3. Audit Layer (Critic)
"""

import time
import json
from typing import Optional, Dict, Any, List
from datetime import datetime
from ..core.schema import (
    GeneratedSample,
    ReasoningTrace,
    CriticEvaluation,
    GenerationConfig,
    PipelineStats
)
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SyndgenPipeline:
    """Main pipeline class for synthetic data generation"""

    def __init__(self, config: GenerationConfig):
        self.config = config
        self.stats = PipelineStats()
        self.memory_buffer = set()  # For tracking generated topics to ensure diversity

    def generate_sample(self, seed: Optional[str] = None) -> GeneratedSample:
        """
        Generate a single data sample through the tri-stage pipeline

        Args:
            seed: Optional seed input for generation

        Returns:
            GeneratedSample with complete data
        """
        start_time = time.time()

        # Stage 1: Seed Layer
        seed_data = self._seed_layer(seed)

        # Stage 2: Inference Layer (Generator)
        scenario, reasoning_trace = self._inference_layer(seed_data)

        # Create initial sample
        sample = GeneratedSample(
            seed=seed_data,
            scenario=scenario,
            reasoning_trace=reasoning_trace,
            final_output=self._generate_final_output(scenario, reasoning_trace),
            metadata={"generation_time": time.time() - start_time}
        )

        # Stage 3: Audit Layer (Critic)
        evaluation = self._audit_layer(sample)

        # Update sample based on evaluation
        sample.is_valid = evaluation.passes_validation
        sample.reasoning_trace.logic_score = evaluation.logic_score

        # Update stats
        self._update_stats(sample, evaluation)

        return sample

    def _seed_layer(self, seed: Optional[str]) -> str:
        """
        Seed Layer: Process seed input or generate from scratch

        Args:
            seed: Optional seed input

        Returns:
            Processed seed data
        """
        if seed:
            # Use provided seed
            return seed
        else:
            # Generate a basic seed (in real implementation, this would use LLM)
            return "Generate a sample question-answer pair about machine learning concepts"

    def _inference_layer(self, seed: str) -> tuple[str, ReasoningTrace]:
        """
        Inference Layer: Generate scenario and reasoning trace

        Args:
            seed: Seed input

        Returns:
            Tuple of (scenario, reasoning_trace)
        """
        # In a real implementation, this would call the DeepSeek R1 1.5B model
        # For now, we'll simulate the output

        # Simulate reasoning trace
        reasoning_steps = [
            "First, I need to understand what machine learning concepts would be appropriate",
            "I recall that supervised learning is a fundamental concept",
            "Supervised learning involves training models on labeled data",
            "Common algorithms include decision trees, SVM, and neural networks",
            "I should create a question about the difference between these algorithms"
        ]

        reasoning_trace = ReasoningTrace(
            thoughts=reasoning_steps,
            confidence_score=0.85
        )

        # Simulate scenario generation
        scenario = (
            "The user is studying for a machine learning exam and needs to understand "
            "the differences between various supervised learning algorithms."
        )

        return scenario, reasoning_trace

    def _generate_final_output(self, scenario: str, reasoning_trace: ReasoningTrace) -> str:
        """
        Generate final output based on scenario and reasoning

        Args:
            scenario: Generated scenario
            reasoning_trace: Reasoning trace

        Returns:
            Final generated output
        """
        # In real implementation, this would be generated by the LLM
        # For simulation, we'll create a sample Q&A pair

        return (
            "Question: What are the key differences between decision trees, "
            "support vector machines (SVM), and neural networks in supervised learning?\n\n"
            "Answer: Decision trees make decisions based on hierarchical feature splits and "
            "are interpretable but prone to overfitting. Support vector machines find the "
            "optimal hyperplane to separate classes and work well with high-dimensional data. "
            "Neural networks consist of interconnected layers that can learn complex patterns "
            "but require more data and computational resources."
        )

    def _audit_layer(self, sample: GeneratedSample) -> CriticEvaluation:
        """
        Audit Layer: Critic evaluation of generated sample

        Args:
            sample: Generated sample to evaluate

        Returns:
            CriticEvaluation with scores and feedback
        """
        evaluation_start = time.time()

        # Simulate critic evaluation (in real implementation, this would use LLM)
        # For now, we'll implement basic logic checks

        # Check if output contains both question and answer
        has_question = "Question:" in sample.final_output
        has_answer = "Answer:" in sample.final_output

        # Check if reasoning trace is coherent
        reasoning_coherent = len(sample.reasoning_trace.thoughts) >= 3

        # Calculate scores
        logic_score = 5 if (has_question and has_answer and reasoning_coherent) else 3
        coherence_score = 5 if reasoning_coherent else 3

        # Determine if passes validation
        passes_validation = logic_score >= self.config.rejection_threshold

        # Generate feedback
        feedback = []
        if not has_question:
            feedback.append("Missing clear question in output")
        if not has_answer:
            feedback.append("Missing clear answer in output")
        if not reasoning_coherent:
            feedback.append("Reasoning trace lacks sufficient depth")

        if passes_validation:
            feedback.append("Sample meets quality criteria")
        else:
            feedback.append("Sample fails to meet quality criteria")

        feedback_text = "; ".join(feedback) if feedback else "No specific feedback"

        evaluation_time = time.time() - evaluation_start

        # Ensure evaluation time is always positive (minimum 0.001s)
        evaluation_time = max(evaluation_time, 0.001)

        return CriticEvaluation(
            sample_id=sample.id,
            logic_score=logic_score,
            coherence_score=coherence_score,
            feedback=feedback_text,
            passes_validation=passes_validation,
            evaluation_time=evaluation_time
        )

    def _update_stats(self, sample: GeneratedSample, evaluation: CriticEvaluation):
        """Update pipeline statistics"""
        self.stats.total_generated += 1
        if sample.is_valid:
            self.stats.total_valid += 1
        else:
            self.stats.total_rejected += 1

        # Update rejection rate
        if self.stats.total_generated > 0:
            self.stats.rejection_rate = self.stats.total_rejected / self.stats.total_generated

        # Update average times (simple moving average)
        gen_time = sample.metadata.get('generation_time', 0)
        eval_time = evaluation.evaluation_time

        if self.stats.total_generated == 1:
            self.stats.avg_generation_time = gen_time
            self.stats.avg_evaluation_time = eval_time
        else:
            # Exponential moving average
            alpha = 0.1  # Smoothing factor
            self.stats.avg_generation_time = (
                alpha * gen_time +
                (1 - alpha) * self.stats.avg_generation_time
            )
            self.stats.avg_evaluation_time = (
                alpha * eval_time +
                (1 - alpha) * self.stats.avg_evaluation_time
            )

    def generate_batch(self, batch_size: int = 10, seed: Optional[str] = None) -> List[GeneratedSample]:
        """
        Generate a batch of samples

        Args:
            batch_size: Number of samples to generate
            seed: Optional seed for all samples

        Returns:
            List of generated samples
        """
        samples = []
        for i in range(batch_size):
            logger.info(f"Generating sample {i+1}/{batch_size}")
            sample = self.generate_sample(seed)

            # If sample fails validation and retries are allowed, try again
            if not sample.is_valid and self.config.max_retries > 0:
                for retry in range(self.config.max_retries):
                    logger.info(f"Retry {retry+1} for sample {i+1}")
                    retry_sample = self.generate_sample(seed)
                    if retry_sample.is_valid:
                        sample = retry_sample
                        break

            samples.append(sample)

        return samples

    def get_stats(self) -> Dict[str, Any]:
        """Get current pipeline statistics as dictionary"""
        stats_dict = self.stats.dict()
        stats_dict['runtime'] = (
            (self.stats.end_time - self.stats.start_time).total_seconds()
            if self.stats.end_time else
            (datetime.now() - self.stats.start_time).total_seconds()
        )
        return stats_dict

    def reset_stats(self):
        """Reset pipeline statistics"""
        self.stats = PipelineStats()
